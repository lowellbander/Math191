\documentclass[11pt]{article}

% \setlength{\topmargin}{9pt}
% \setlength{\textheight}{9in}
% \setlength{\headheight}{2pt}
% \setlength{\headsep}{4pt}
% \setlength{\oddsidemargin}{0.25in}
% \setlength{\textwidth}{6in}
% \pagestyle{plain}


\usepackage{amsmath,amssymb,amsrefs}
\usepackage{amscd}
\usepackage{latexsym}
\usepackage{graphics}
\usepackage{amssymb}


\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\addtolength{\topmargin}{-1in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}

\usepackage{algorithm}
\usepackage{algorithmic}

% \setlength{\topmargin}{0in}
% \setlength{\headheight}{0in}
% \setlength{\headsep}{0in}
% \setlength{\textheight}{7.7in}
% \setlength{\textwidth}{6.5in}
% \setlength{\oddsidemargin}{0in}
% \setlength{\evensidemargin}{0in}
% \setlength{\parindent}{0.25in}
% \setlength{\parskip}{0.25in}

\usepackage{subfigure}
\usepackage{graphicx}

%
% this command enables to remove a whole part of the text 
% from the printout
% to use it just enter
% \remove{  
% before the text to be excluded and
% } 
% after the text
\newcommand{\remove}[1]{}

%
% The following macros are used to generate nice code for programs.
% See example on how to use it below
%

%%%%%%%%%%%%%%%%%%%%% program macros %%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%% End of PROGRAM macros %%%%%%%%%%%%%%%%%



\newcommand{\lecture}[5]{
   \pagestyle{headings}
   \thispagestyle{plain}
   \newpage
%   \setcounter{chapter}{#1}
%   \setcounter{page}{#2}
%  \set\thechapter{#3}
   \noindent
   \begin{center}
   \framebox{
      \vbox{
    \hbox to 6.28in { {\bf MATH 191 Topics in Data Science: Algorithms and Math. Foundations
                        \hfill  - Fall 2015} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #3  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #4 \hfill Scribes: #5} }
      }
   }
   \end{center}
   \markboth{Lecture #1: #3}{Lecture #1: #3}
   \vspace*{4mm}
}

%
% Use these macros for organizing sections of your notes.
% Each command takes two arguments: (1) the title of the section and and
% (2) a keyword for that section to appear in the index.  (See examples.)
% Please don't use \section, \subsection, and \subsubsection directly!
%

\newcommand{\topic}[2]{\section{#1} \index{#2} \markright{#1}}
\newcommand{\subtopic}[2]{\subsection{#1} \index{#2}}
\newcommand{\subsubtopic}[2]{\subsubsection{#1} \index{#2}}
 
%
% Convention for citations is first author's last name followed by other
% authors' last initials, followed by the year.  For example, to cite the
% seventh entry in the course bibliography, you would type: \cite{BurnsL80}
% (To avoid bibliography problems, for now we redefine the \cite command.)
%

\renewcommand{\cite}[1]{[#1]}

%
% These are just to make things a little easier:
%
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\blank}{\vspace{1ex}}   % generates a blank line in the output

%
% Use these for theorems, lemmas, proofs, etc.
%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newcommand{\qed}{\hfill $\Box$}
% \newenvironment{proof}{\par{\bf Proof:}}{\qed \par}
\newenvironment{proof}{{\em Proof:}}{\hfill\rule{2mm}{2mm}}

%
% Use the following for definitions.
% \bigdef is for definitions to be set off by themselves; \smalldef is for
% definitions given in the middle of a paragraph.
%
\newenvironment{dfn}{{\vspace*{1ex} \noindent \bf Definition }}{\vspace*{1ex}}
\newcommand{\bigdef}[2]{\index{#1}\begin{dfn} {\rm #2} \end{dfn}}
\newcommand{\smalldef}[1]{\index{#1} {\em #1}}
% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:
% \usepackage{subfigure}
% \usepackage{graphicx}

\begin{document}

\lecture{4}{1}{October 2, 2015}{Mihai Cucuringu}{Eric Aberbook \& Lowell Bander}

\section{Bias-Variance Decomposition}

If we let $z$ be a random variable, then we can express the variance of $z$ as follows.

\begin{align}
Var(z) &= E[(z-E(z))^2]\\
&= E[z^2 - 2zE(z) + (E(z))^2]\\
&= E(z^2) - 2E(zE(z)) + E[E(z)^2]\\
&= E(z^2) - 2(E(z))^2 + (E(z))^2\\
&= E(z^2) - (E(z))^2
\end{align}

\begin{lemma}
\begin{align}
E(z^2) = E[(z-E(z)^2)]+[E(z)^2]
\end{align}
\end{lemma}

\begin{proof}


Not yet sure how things fit together, so just gonna transcribe the equations I have for now.\\

\begin{align}
E(z^2) &= E[(z - E(z))^2]
\end{align}

\begin{align}
y = f(x) + \epsilon : \epsilon \sim N(0, \Delta^2) % very low confidence on the second parameter here.
\end{align}

Let $\{(x_i, y_i) | i = 1, N\}$ be the training sample. We want to fit some hypothesis function $h(x)$ [e.g. $h(x) = ax + b$] such that we minimize the squared error represented by $\sum_{i=1}^{N} (y_i - h(x_i))^2$. \\

Imagine a new test points coming in represented by $(x_o, y_o)$ such that they are distributed by a probability density function $p$.\\

Observe that $y_o = f(x_o) + \epsilon$.\\

Our goal is to minimize reducible error, so let us express the reducible error in terms of the bias error and the variance error. If we let $y_o$ be the ground truth and $h(x_o)$ be our forecast, then the reducible error can be expressed as

\begin{align}
E[(y_o - h(x_o))^2] &= E[y_o^2 - 2h(x_o)y_o + (h(x_o))^2]\\
&=E[y_o^2] - 2E[h(x_o)\cdot y_o] + E[(h(x_o))^2]\\
&= E[(y_o - E(y_o))^2] + (E(y_o))^2 - 2\cdot E[h(x_o)\cdot (f(x_o) + \epsilon)] + E[(h(x_o) - \hbar(x_o))^2] + (\hbar(x_o))^2
\end{align}

Since $\epsilon$ has a mean of zero, the term $2\cdot E[h(x_o)\cdot (f(x_o) + \epsilon)]$ can be rewritten as $2E[h(x_o)\cdot f(x_o)]$ which can in turn be rewritten as $2f(x_o)\cdot\hbar(x_o)$. And so, our above decomposition continues as 

\begin{align}
&= E[(y_o - E(y_o))^2] + (E(y_o))^2 - 2f(x_o)\cdot\hbar(x_o) + E[(h(x_o) - \hbar(x_o))^2] + (\hbar(x_o))^2\\
&= E[(y_o - E(y_o))^2] + (E(y_o))^2 - 2f(x_o)\cdot\hbar(x_o) + E[(h(x_o)+E[(h(x_o) - \hbar(x_o))])^2] + (\hbar(x_o))^2\\
&= E(\epsilon^2) + E[(h(x_o) - \hbar(x_o))^2] + (f(x_o))^2 - 2f(x_o)\hbar(x_o) + (\hbar(x_o))^2\\
&= E(\epsilon^2) + E[(h(x_o) - \hbar(x_o))^2] + (f(x_o) - \hbar(x_o))^2\\
&=Var(\epsilon) + Var(h) + Bias(h(x_o))
\end{align}

\end{proof}

\section{Pearson Correlation Measures}

The Pearson correlation, represented by \(\rho\) is the measure of linear dependence between variables.
There are two representations of \(\rho\): \\

Population Correlation:
\begin{align}
Cor(X,Y)
&= \dfrac{Cov(X,Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}
\end{align}

Sample Correlation:
\begin{align}
cor(x,y)
&= \dfrac{cov(x,y)}{\sqrt{var(X)}\sqrt{var(Y)}} \\
&= \dfrac{(x - \bar{x}1)(y - \bar{y}1)}{\| \mathbf{x - \bar{x}1} \|\| \mathbf{y - \bar{y}1} \|}
\end{align}

Note the difference between Population and Sample- \\
Sample- A subset of items from a larger population that is collected to make inferences \\
Population- The total collection of items about which you want to make inferences.



\section{Properties of Population Correlation}
The following are all relevant properties of \(\rho\):

\begin{itemize}
\item $Cor(X,X) = 1$
\item $Cor(X,Y) = Cor(Y,X)$
\item \textit{Cor(aX + b, Y)} = sign(a)Cor(X,Y) for any a,b $\epsilon$ R
\item $-1 \leq \textit{Cor(X,Y)}  \leq 1 $
\item $Cor(X,Y)  =  1$ if and only if $Y = aX + b$ for some a,b $\epsilon \mathbf{R}$ with a $\neq$ 0 
\item If  \textit{(X,Y)} are independent then \textit{Cor(X,Y)} = 0
%\item 5



\end{itemize}

\section{Bivariate Normal Distribution}
\section{Properties of Sample Correlation}
\section{Drawbacks of Pearson Correlation}

\end{document}